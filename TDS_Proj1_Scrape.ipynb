{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scrape Using GitHub API"
      ],
      "metadata": {
        "id": "5wunnpbZLR2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import requests\n",
        "\n",
        "def search_users_in_bangalore():\n",
        "    url = \"https://api.github.com/search/users?q=followers%3A%3E100+location%3ABanglore&ref=searchresults&s=followers&type=Users&per_page=100\"\n",
        "    #params = {\"q\": \"location:'Banglore'%2Bfollowers:>100\", \"per_page\": 100,\"type\":\"Users\"}\n",
        "    users_with_min_followers = []\n",
        "\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "    return data['items']\n",
        "\n",
        "users = search_users_in_bangalore()\n",
        "print(users)\n",
        "\n",
        "def fetch_users_from_github():\n",
        "\n",
        "  users_data = []\n",
        "  for user_ind in users:\n",
        "    user_url = user_ind['url'];\n",
        "    user_response = requests.get(user_url)\n",
        "    user = user_response.json()\n",
        "    user_info = {\n",
        "        'login': user['login'],\n",
        "        'id': user['id'],\n",
        "        'name': user['name'],\n",
        "        'company': user['company'],\n",
        "        'blog': user['blog'],\n",
        "        'location': user['location'],\n",
        "        'email': user['email'],\n",
        "        'hireable': user['hireable'],\n",
        "        'bio': user['bio'],\n",
        "        'public_repos': user['public_repos'],\n",
        "        'followers': user['followers'],\n",
        "        'following': user['following'],\n",
        "        'created_at': user['created_at']\n",
        "    }\n",
        "    users_data.append(user_info)\n",
        "  return pd.DataFrame(users_data)\n",
        "\n",
        "df = fetch_users_from_github()\n",
        "print(df)\n",
        "\n",
        "df.to_csv('users.csv')\n",
        "\n",
        "def fetch_repos_from_github():\n",
        "\n",
        "  repo_data = []\n",
        "  for user_ind in users:\n",
        "    repo_url = user_ind['repos_url'];\n",
        "    repo_response = requests.get(repo_url)\n",
        "    repos = repo_response.json()\n",
        "    for repo in repos:\n",
        "      repo_info = {\n",
        "          'login': user_ind['login'],\n",
        "          'id': repo['id'],\n",
        "          'full_name': repo['full_name'],\n",
        "          'stargazers_count': repo['stargazers_count'],\n",
        "          'watchers_count': repo['watchers_count'],\n",
        "          'language': repo['language'],\n",
        "          'has_projects': repo['has_projects'],\n",
        "          'has_wiki': repo['has_wiki'],\n",
        "          'license_name': repo['license'],\n",
        "          'created_at': repo['created_at']\n",
        "      }\n",
        "      repo_data.append(repo_info)\n",
        "  return pd.DataFrame(repo_data)\n",
        "\n",
        "df_repo = fetch_repos_from_github()\n",
        "print(df_repo)\n",
        "\n",
        "df_repo.to_csv('repositories.csv')"
      ],
      "metadata": {
        "id": "Oel0e3ywLYv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GITHUB_TOKEN = '<<GITHUB TOKEN>>'\n",
        "\n",
        "def check_rate_limit():\n",
        "    url = 'https://api.github.com/rate_limit'\n",
        "    headers = {'Authorization': f'token {GITHUB_TOKEN}'}\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        rate_limit_info = response.json()\n",
        "        core_limit = rate_limit_info['resources']['core']\n",
        "        search_limit = rate_limit_info['resources']['search']\n",
        "\n",
        "        print(\"Core Rate Limit:\")\n",
        "        print(f\"- Remaining: {core_limit['remaining']}\")\n",
        "        print(f\"- Reset: {core_limit['reset']}\")\n",
        "\n",
        "        print(\"Search Rate Limit:\")\n",
        "        print(f\"- Remaining: {search_limit['remaining']}\")\n",
        "        print(f\"- Reset: {search_limit['reset']}\")\n",
        "    else:\n",
        "        print(f\"Failed to check rate limit: {response.status_code} - {response.text}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    check_rate_limit()"
      ],
      "metadata": {
        "id": "teZeGktCLglV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrape Using PyGithub"
      ],
      "metadata": {
        "id": "E2l7xo9sLtYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyGithub"
      ],
      "metadata": {
        "id": "8wGnXA-KO98m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from github import Github as PyGithubGithub\n",
        "\n",
        "GITHUB_TOKEN = '<<GITHUB TOKEN>>'\n",
        "github_instance = PyGithubGithub(GITHUB_TOKEN)\n",
        "\n",
        "def fetch_users_with_criteria():\n",
        "    page = 1\n",
        "    url = 'https://api.github.com/search/users'\n",
        "    users_data = []\n",
        "\n",
        "    while True:\n",
        "        q_params = {\n",
        "            'q': 'location:Bangalore followers:>100',\n",
        "            'per_page': 100,\n",
        "            'page': page\n",
        "        }\n",
        "        response = requests.get(url, headers={\"Authorization\": f\"token {GITHUB_TOKEN}\"}, params=q_params)\n",
        "        data = response.json()\n",
        "\n",
        "        if 'items' not in data or not data['items']:\n",
        "            break\n",
        "\n",
        "        users_data.extend(data['items'])\n",
        "        page += 1\n",
        "\n",
        "    return users_data\n",
        "\n",
        "def strip_company_name(companyName):\n",
        "    if companyName:\n",
        "        return companyName.strip().lstrip('@').upper()\n",
        "    return companyName\n",
        "\n",
        "def fetch_users_from_github(user):\n",
        "    user_data = []\n",
        "    repo_data = []\n",
        "\n",
        "    try:\n",
        "        user_details = github_instance.get_user(user['login'])\n",
        "\n",
        "        company_corr = strip_company_name(user_details.company)\n",
        "\n",
        "        user_data = [\n",
        "            user_details.login,\n",
        "            user_details.name,\n",
        "            company_corr,\n",
        "            user_details.location,\n",
        "            user_details.email,\n",
        "            user_details.hireable,\n",
        "            user_details.bio,\n",
        "            user_details.public_repos,\n",
        "            user_details.followers,\n",
        "            user_details.following,\n",
        "            user_details.created_at\n",
        "        ]\n",
        "\n",
        "        # We don't need to fetch more than 500 repos so stop at 500\n",
        "        repos = user_details.get_repos()\n",
        "        for i, repo in enumerate(repos):\n",
        "            if i >= 500:\n",
        "                break\n",
        "            repo_data.append([\n",
        "                user_details.login,\n",
        "                repo.full_name,\n",
        "                repo.created_at,\n",
        "                repo.stargazers_count,\n",
        "                repo.watchers_count,\n",
        "                repo.language,\n",
        "                repo.has_projects,\n",
        "                repo.has_wiki,\n",
        "                repo.license.name if repo.license else None\n",
        "            ])\n",
        "\n",
        "    except Exception as exp:\n",
        "        print(f\"{user['login']}: {exp}\")\n",
        "\n",
        "    return user_data, repo_data\n",
        "\n",
        "def start():\n",
        "\n",
        "    # First fetch users in Banagalore with > 100 followers\n",
        "    users_data = fetch_users_with_criteria()\n",
        "\n",
        "    user_list = []\n",
        "    repo_list = []\n",
        "\n",
        "    # Iterate over all the users and get their user details and repos\n",
        "    # Considering this is overwhelming API calls, implement TPE\n",
        "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
        "        results = list(executor.map(fetch_users_from_github, users_data))\n",
        "\n",
        "        for user_data, repo_data in results:\n",
        "            if user_data:\n",
        "                user_list.append(user_data)\n",
        "            if repo_data:\n",
        "                repo_list.extend(repo_data)\n",
        "\n",
        "    # Create CSV for users\n",
        "    users_df = pd.DataFrame(user_list, columns=[\n",
        "        'login', 'name', 'company', 'location', 'email', 'hireable',\n",
        "        'bio', 'public_repos', 'followers', 'following', 'created_at'])\n",
        "\n",
        "    users_df.to_csv('users.csv', index=False) # create without indexes since we dont need them in the CSV first column\n",
        "\n",
        "    # Create CSV for repositories\n",
        "    repos_df = pd.DataFrame(repo_list, columns=[\n",
        "        'login', 'full_name', 'created_at', 'stargazers_count',\n",
        "        'watchers_count', 'language', 'has_projects', 'has_wiki', 'license_name'])\n",
        "\n",
        "    repos_df.to_csv('repositories.csv', index=False) # create without indexes since we dont need them in the CSV first column\n",
        "\n",
        "start()\n",
        "print(\"!! All Done !!\")"
      ],
      "metadata": {
        "id": "2CmKwGySLwGI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}